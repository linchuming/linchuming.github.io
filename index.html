<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chuming Lin (林楚铭)</title>
  
  <meta name="author" content="Chuming Lin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Chuming Lin (林楚铭)</font></strong></name>
              </p>
              <p>Chuming is a Researcher at Tencent Youtu Lab, Shanghai.
              </p>
              He got his Master degree from School of Computer Science, Fudan University in 2020, supervised by Prof. 
              <a href="http://homepage.fudan.edu.cn/boyan/">Bo Yan</a>. Previously, he received the Bachelor degree from Fudan University in 2017.
              <p>
              <p>
                His research interests include computer vision and deep learning, particularly focusing on temporal action localization and image/video enhancement tasks.
              </p>

              <p style="text-align:center">
                <a href="mailto:linchuming22@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=6pS2epEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/linchuming/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chuming-lin-aa4967119/">LinkedIn</a> 
              </p>
            </td>
            
            <!-- <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./figures/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./figures/me.jpg" class="hoverZoomLink"></a>
            </td> -->
          </tr>
        </tbody></table>
        
<p style="text-align:left"><strong><font size="4px">Recent news</font></strong></p> 
		<ul>
            <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution.</p></li>
            <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the runner-up of CVPR NTIRE 2021 Challenge on Video Spatial Super-Resolution.</p></li>
            <li><p style="text-align:left">03/2021 – Our <a href="https://arxiv.org/pdf/2103.13137.pdf"; style="color: #EE7F2D;"> <b>AFSD</b></a> on temporal action localization is accepted by CVPR'21, with the acceptance rate to be <b>23.7%</b>. </p></li>
            <li><p style="text-align:left">11/2019 – Our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> on temporal action proposal is accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/Tencent/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge on Temporal Action Proposals</b></a>. </p></li>
            <li><p style="text-align:left">11/2018 – Our <a href="https://arxiv.org/pdf/1909.13057.pdf"; style="color: #EE7F2D;"> <b>FFCVSR</b></a> on video super-resolution is accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b>. </p></li>

	    </ul>
	 
	<br>
	<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>#</sup> first student author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./figures/AFSD.png" style="height: 120px; width: 250px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2103.13137">
                  <papertitle><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></papertitle>
                  </a>
                  <br>
                  <b>C. Lin*</b>, C. Xu*, D. Luo, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang and Y. Fu.              
                  <br>
                  <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
                  <br>
                  <a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
              /
              <a href="https://github.com/TencentYoutuResearch/ActionDetection-AFSD"; style="color: #EE7F2D;">Code (Official)</a>
                  </p>
                </td><tr>	

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./figures/FGME.png" style="height: 120px; width: 250px; margin-top: 10px;"></td>
                <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9234068/"><papertitle><b>Fine-Grained Motion Estimation for Video Frame Interpolation</b></papertitle></a><br>
                B. Yan, W. Tan, <b>C. Lin<sup>#</sup></b>, L. Shen.
                <br>
                <em>IEEE Transactions on Broadcasting (<b>IEEE Trans. Broadcast.</b>)</em>, 2020
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9234068/"; style="color: #EE7F2D;">Paper</a>
                </p>
                </td></tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./figures/DBG.png" style="height: 120px; width: 250px; margin-top: 10px"></td>
                <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1911.04127"><papertitle><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></papertitle></a><br>
                <b>C. Lin*</b>, J. Li*, Y. Wang, Y. Tai, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
                <br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1911.04127"; style="color: #EE7F2D;">arXiv</a>
                /
                <a href="https://github.com/Tencent/ActionDetection-DBG"; style="color: #EE7F2D;">Code (Official)</a>
                <br>
                <p><font color="red">No. 1 performace on<a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b> ActivityNet Challenge on Temporal Action Proposals</b></a></font></p> 
                </p>
                </td></tr>

                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="./figures/FFCVSR.png" style="height: 120px; width: 200px; margin-top: 10px; margin-left: 30px"></td>
                    <td style="padding:10px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/1909.13057"><papertitle><b>Frame and Feature-Context Video Super-Resolution</b></papertitle></a><br>
                    B. Yan, <b>C. Lin<sup>#</sup></b>, W. Tan.
                    <br>
                    <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
                    <br>
                    <a href="https://arxiv.org/abs/1909.13057"; style="color: #EE7F2D;">arXiv</a>
                    /
                    <a href="https://github.com/linchuming/FFCVSR"; style="color: #EE7F2D;">Code (Official)</a>
                    </p>
                    </td></tr>
                
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="./figures/CycleIR.png" style="height: 120px; width: 250px; margin-top: 10px;"></td>
                    <td style="padding:10px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/1905.03556"><papertitle><b>Cycle-IR: Deep Cyclic Image Retargeting</b></papertitle></a><br>
                    W. Tan, B. Yan, <b>C. Lin</b>, X. Niu.
                    <br>
                    <em>IEEE Transactions on Multimedia (<b>TMM</b>)</em>, 2019
                    <br>
                    <a href="https://arxiv.org/abs/1905.03556"; style="color: #EE7F2D;">arXiv</a>
                    /
                    <a href="https://github.com/mintanwei/Cycle-IR"; style="color: #EE7F2D;">Code (Official)</a>
                    </p>
                    </td></tr>

                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="./figures/FCSN.png" style="height: 120px; width: 250px; margin-top: 10px;"></td>
                    <td style="padding:10px;width:75%;vertical-align:middle">
                    <a href="https://ieeexplore.ieee.org/abstract/document/8451816"><papertitle><b>Foreground Detection in Surveillance Video with Fully Convolutional Semantic Network</b></papertitle></a><br>
                    <b>C. Lin</b>, B. Yan, W. Tan.
                    <br>
                    <em>IEEE International Conference on Image Processing (<b>ICIP</b>)</em>, 2018
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/8451816"; style="color: #EE7F2D;">Paper</a>
                    </p>
                    </td></tr>

			
        </table></p>
 
		
	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">2021 Winner Award of CVPR NTIRE 2021 Challenge on Video Super-Resolution: Spatial-Temporal (Team name: Imagination)</p></li>
            <li><p style="text-align:left">2021 Runner-Up Award of CVPR NTIRE 2021 Challenge on Video Super-Resolution: Spatial (Team name: Imagination)</p></li>
            <li><p style="text-align:left">2020 Shanghai outstanding graduates</p></li>
            <li><p style="text-align:left">2018 and 2019 Huawei Graduate Scholarship of Fudan University</p></li>
            <li><p style="text-align:left">2017 2nd Prize of Graduate Mathematical Contest in Modeling, China</p></li>
            <li><p style="text-align:left">2013 1st Prize of National Olympiad in Informatics in Province (NOIP)</p></li>
		</ul><br>


    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=dzrmtbX9b1i25X8BotKZuBwiqyozTKQUPPVGJa3kUww"></script>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:center;">
            For the style of my personal website, please refer to the wonderful page from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
            <br>
            </p>
          </td>
        </tr>
      </tbody></table>
</body>          

</html>